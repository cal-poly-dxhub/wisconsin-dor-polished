# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients.baml": "client<llm> ClaudeSonnetBedrock {\n  provider aws-bedrock\n  options {\n    model \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n    region \"us-west-2\"\n    inference_configuration {\n      max_tokens 2048\n      temperature 0.0\n      top_p 0.9\n    }\n  }\n}\n",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"go\", \"rust\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../packages/messages/lambdas/streaming\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.218.1\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
    "rag_response.baml": "// Data models for chat history and documents\nclass ChatTurn {\n  query string\n  answer string\n}\n\nclass Document {\n  document_id string\n  title string\n  content string\n  source string?\n  source_id string?\n}\n\n// RAG Response Generation Function\nfunction GenerateRAGResponse(\n  history: ChatTurn[],\n  documents: Document[],\n  query: string\n) -> string {\n  client ClaudeSonnetBedrock\n\n  prompt #\"\n    Prior conversation history: {{ history }}. Documents (including FAQs): {{ documents }}. User query: {{ query }}.\n\n    CORE RESPONSIBILITIES: You are a helpful assistant that is familiar with the Wisconsin Department of Revenue. Answer questions about Department of Revenue using both the FAQ and additional knowledge base content. Provide accurate information that builds upon the FAQ response. Be warm, professional, and supportive in all interactions. Maintain conversation context across multiple exchanges\n\n    RESPONSE GUIDELINES: Be conversational and helpful, not robotic. Provide specific details when available. Structure responses clearly with relevant details. Double-check contact information for accuracy. Suggest related resources or next steps when appropriate. Always prioritize accuracy, helpfulness, and user experience in your responses.\n\n    Don't say phrases like 'based on the provided resources', 'according to the documents', 'the FAQ indicates', 'the materials show', or similar references to source materials. **If the question lacks sufficient detail to provide an accurate answer, ask a clear and polite follow-up question to gather the necessary information.** Format your response as Markdown.\n  \"#\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // You can also use custom LLM params with a custom client name from clients.baml like \"client CustomGPT5\" or \"client CustomSonnet4\"\n  client \"openai-responses/gpt-5-mini\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return _file_map